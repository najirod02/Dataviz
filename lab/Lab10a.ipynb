{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction examples\n",
    "Shahryar Noei\n",
    "\n",
    "Originally designed by Marco Chierici & Giuseppe Jurman\n",
    "\n",
    "May 13, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising high-dimensional datasets using PCA and t-SNE\n",
    "\n",
    "We will use the MNIST-dataset in this write-up. \n",
    "There is no need to download the dataset manually as we can grab it through using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start by loading in the MNIST original data from the default `$HOME/scikit_learn_data/openml/` location (the data will be downloaded the first time the `fetch_openml` function is called):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following will take a while:\n",
    "mnist = fetch_openml('mnist_784', parser='auto')\n",
    "df = mnist.data / 255.0\n",
    "y = mnist.target\n",
    "print(df.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already provided as Pandas DataFrame. For convenience, we add the labels as a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = df.columns\n",
    "df['y'] = y.cat.codes\n",
    "print('Size of the dataframe: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't want to use 70,000 digits in all calculations, we take a random subset of the digits. \n",
    "\n",
    "We create a random permutation of numbers from 0 to 69,999, which allows us later to select the first five or ten thousand for our calculations and visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rndperm = np.random.permutation(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our dataframe and our randomisation vector ready. Let's first check what these numbers actually look like. To do this we generate plots of 15 randomly selected images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "for i in range(0, 15):\n",
    "    ax = fig.add_subplot(3, 5, i+1, title=f\"Digit: {df.loc[rndperm[i],'y']}\")\n",
    "    ax.matshow(df.loc[rndperm[i], feat_cols].values.reshape((28, 28)).astype(float))#matshow to show a matrix (even imgshow)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start thinking about how we can actually distinguish the zeros from the ones and twos, and so on. If you were, for example, a post office such an algorithm could help you read and sort the handwritten envelopes using a machine instead of having humans do that. Obviously, nowadays there are very advanced methods to do this, but this dataset still provides a very good testing ground for seeing how specific methods for dimensionality reduction work and how well they work.\n",
    "\n",
    "The images are all 28-by-28 pixel images: therefore, we have a total of 784 variables (\"dimensions\"), each holding the value of one specific pixel.\n",
    "\n",
    "What we can do is reduce the number of dimensions drastically whilst trying to retain as much of the variation in the data as possible. We start out with Principal Component Analysis.\n",
    "\n",
    "Initially we generate, from the original 784 dimensions, the first three principal components. We also check how much of the variation in the total dataset they actually account for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that it is important to centralize and standardize the values\n",
    "# otherwise, axis will have different \"meaning\"\n",
    "\n",
    "# instantiate a PCA object\n",
    "pca = PCA(n_components=3)\n",
    "# compute PCA and transform the data\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "# save to dataframe\n",
    "df['pca-one'] = pca_result[:, 0]\n",
    "df['pca-two'] = pca_result[:, 1] \n",
    "df['pca-three'] = pca_result[:, 2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()\n",
    "#this is the total variance retained from the 3 componnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that the first three components account for 23% of the variation in the entire dataset, let's see if that is enough to visually set the different digits apart. \n",
    "\n",
    "We can create a scatterplot of the first and second principal component, and color each of the different types of digits with a different color. \n",
    "If these principal components capture enough variation, we can suppose that the same type of digits would be positioned (i.e., clustered) together in groups, which would mean that the first two principal components actually tell us a great deal about the specific types of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    data=df,\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    # legend=\"full\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 3D version of the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 14))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.scatter(\n",
    "    xs=df[\"pca-one\"], \n",
    "    ys=df[\"pca-two\"], \n",
    "    zs=df[\"pca-three\"], \n",
    "    c=df[\"y\"], \n",
    "    cmap='tab10'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that the components definitely hold some information, especially for specific digits, but clearly not enough to set all of them apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "Let's see if t-SNE can be of help here: we will first try to run the algorithm on the actual dimensions of the data (784) and see how it performs. \n",
    "\n",
    "To make sure we don't burden our machine in terms of memory and power/time we will **only use the first 10,000 samples** to run the algorithm on. To compare later on, we also run the PCA again on this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "df_subset = df.loc[rndperm[:N], :].copy()\n",
    "data_subset = df_subset[feat_cols].values\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(data_subset)\n",
    "\n",
    "df_subset['pca-one'] = pca_result[:, 0]\n",
    "df_subset['pca-two'] = pca_result[:, 1] \n",
    "df_subset['pca-three'] = pca_result[:, 2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the two resulting dimensions, we can again visualise them by creating a scatterplot and coloring each sample by its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['tsne-2d-one'] = tsne_results[:, 0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:, 1]\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_subset,\n",
    "    # legend=\"full\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a significant improvement over the PCA visualisation we used earlier: we can see that the digits are very clearly clustered in their own sub groups. This means that, for this particular dataset, accounting for variation is not a good way to separate the different classes.\n",
    "\n",
    "If we now used a clustering algorithm to pick out the separate clusters, we could probably quite accurately assign new points to a label.\n",
    "\n",
    "Just to compare PCA with t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_subset,\n",
    "    alpha=0.3,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_subset,\n",
    "    alpha=0.3,\n",
    "    ax=ax2,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have obtained the t-SNE projection from all of the 784 input dimensions. We might wonder what would happen if we tried **using less information.**\n",
    "\n",
    "We further reduce the number of dimensions before feeding the data into the t-SNE algorithm. To this aim, we use PCA again: \n",
    "\n",
    "1. First, we create a new dataset containing the 50 dimensions generated by the PCA reduction algorithm;\n",
    "1. Then, we use this dataset as input to t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_50 = PCA(n_components=50)\n",
    "pca_result_50 = pca_50.fit_transform(data_subset)\n",
    "\n",
    "print('Cumulative explained variation for 50 principal components: {}'.format(pca_50.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **scree plot** shows the explained variance of each principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_values = np.arange(pca_50.n_components_) + 1\n",
    "plt.bar(pc_values, pca_50.explained_variance_ratio_)\n",
    "# or:\n",
    "# plt.plot(pc_values, pca_50.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "plt.title('Scree plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 50 components roughly hold around 83% of the total variation in the data.\n",
    "Now let's try and feed this data into the t-SNE algorithm.\n",
    "Again, we'll use 10,000 samples out of the 70,000 to make sure the algorithm does not take up too much memory and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, init='random', learning_rate=200)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result_50)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['tsne-pca50-one'] = tsne_pca_results[:, 0]\n",
    "df_subset['tsne-pca50-two'] = tsne_pca_results[:, 1]\n",
    "\n",
    "my_palette = sns.color_palette('hls', 10)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=my_palette,\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=my_palette,\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-pca50-one\", y=\"tsne-pca50-two\",\n",
    "    hue=\"y\",\n",
    "    palette=my_palette,\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax3\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can clearly see how all the samples are nicely spaced apart and grouped together with their respective digits.\n",
    "\n",
    "Note that even reducing the dimensionality that much, we get a good separation of the digits. This implies that you could reduce a lot the dimensionality (and thus, the computational burden) while still obtaining a reasonable separation, which can then be the starting point for a clustering algorithm or for another algorithm (e.g. something like a Neural Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming new data with UMAP\n",
    "\n",
    "UMAP is useful for generating visualisations, but if you want to **make use of UMAP more generally for machine learning tasks** it is important to be able to train a model and then later pass new data to the model and have it transform that data into the learned space. \n",
    "\n",
    "For example, if we use UMAP to learn a latent space and then train a classifier on data transformed into the latent space, then the classifier is only useful for prediction if we can transform data for which we want a prediction into the latent space the classifier uses. Fortunately, UMAP makes this possible, albeit more slowly than some other transformers that allow this.\n",
    "\n",
    "Remember that, differently from t-SNE, **UMAP generates a real full projection** from the original space to the new space. This projection is based on the training points but is extended, as a geometrical map, to the whole space.\n",
    "\n",
    "To install UMAP (and accessory packages) using `pip`: `pip install umap-learn[plot]`\n",
    "\n",
    "To install UMAP (and accessory packages) using `conda`: `conda install umap-learn datashader bokeh holoviews colorcet scikit-image`\n",
    "\n",
    "You'll also need to install or update `ipywidgets`: `pip install ipywidgets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "sns.set(context='notebook', style='white', rc={'figure.figsize':(14, 10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this functionality we'll make use of another classical data set, `digits`, which is included in `scikit-learn` and is derived from the [UCI ML handwritten digits datasets](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `digits` dataset, each sample is a 8x8 image of a digit (hence the 64 dimensions).\n",
    "As we did for the MNIST data, we visualize a few images together with their ground truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "for i in range(0, 15):\n",
    "    ax = fig.add_subplot(3, 5, i+1, title=f\"Digit: {digits.target[i]}\")\n",
    "    ax.matshow(digits.data[i].reshape((8, 8)).astype(float), cmap=plt.cm.gray_r)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of plotting a few images involves building a grid of axes and then looping through them plotting an image into each one in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_array = plt.subplots(10, 10)\n",
    "axes = ax_array.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray_r')\n",
    "\n",
    "plt.setp(axes, xticks=[], yticks=[], frame_on=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resolution of the images is quite low – for the most part we recognize them as digits, but sometimes they are so blurred that it is hard to guess even a human. The zeros stand out as the easiest to pick out. Beyond that, things get a little harder: some of the squashed eights look like ones, some of the threes start to look a little like crossed sevens when drawn badly, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 8x8 image can be flattened into a vector of 64 grayscale values. How much of the digits structure can we discern from these 64 values? \n",
    "\n",
    "At least in principle 64 dimensions is overkill for this task, and we would reasonably expect that there should be some smaller number of \"latent\" features that would be sufficient to describe the data reasonably well.\n",
    "\n",
    "We could try a scatterplot matrix of just the first 10 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(digits.data[:, 1:11])\n",
    "digits_df['digit'] = pd.Series(digits.target).map(lambda x: f'Digit {x}')\n",
    "sns.pairplot(digits_df, hue='digit', palette='Spectral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this approach is not feasible for this data.\n",
    "\n",
    "Let's try UMAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first construct a UMAP object, and then to train our reducer, letting it learn about the manifold.\n",
    "\n",
    "For this, UMAP follows the sklearn API and has a method `fit` which we pass the data we want the model to learn from. Then, the `transform` method will return the transformed data as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(digits.data)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the resulting embedding, coloring the data points by the class that they belong to (the digit they represent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "            c=digits.target, \n",
    "            cmap='Spectral', \n",
    "            s=5)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve UMAP visualizations by using the `umap.plot` module (`pip install umap-learn[plot]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "umap.plot.points(reducer, labels=digits.target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(reducer, labels=digits.target, theme=\"fire\"); # viridis, inferno, ... For more type \"umap.plot.points?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this UMAP experimentation on the whole dataset, we see if we can use UMAP as a data transformed before fitting a machine learning model.\n",
    "\n",
    "To avoid overfitting, we use sklearn `train_test_split` to divide into a training and test set (stratified over the different digit types). By default `train_test_split` will carve off 25% of the data for testing, which seems suitable in this case: otherwise, we can change the ratio through the argument `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data,\n",
    "                                                    digits.target,\n",
    "                                                    stratify=digits.target,\n",
    "                                                    test_size=0.25, # default\n",
    "                                                    random_state=42, # set the random seed for reproducibility\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of what we are looking at, we now train a couple of different classifiers and then see how well they score on the test set. \n",
    "\n",
    "For this example, we fit a support vector classifier (SVC) and a K-nearest neighbor classifier (KNN). Note that ideally we should be tuning hyper-parameters (perhaps a grid search using k-fold cross-validation), but for the purposes of this simple demo we will simply use the default parameters for both classifiers:\n",
    "\n",
    "* SVC - Gaussian (RBF) kernel, `C=1`, `gamma='scale'`, i.e. gamma = $1/\\left(n_{features} * var(X)\\right)$\n",
    "* KNN - 5 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', gamma='scale').fit(X_train, y_train)\n",
    "knn = KNeighborsClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question is how well these classifiers perform on the test set. Conveniently sklearn provides a `matthews_corrcoef` function to compute the Multiclass Matthews Correlation Coefficient (MCC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVC MCC: {matthews_corrcoef(svc.predict(X_test), y_test):.5f}\")\n",
    "print(f\"KNN MCC: {matthews_corrcoef(knn.predict(X_test), y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the max value of MCC is close to one, both classifiers are doing very well.\n",
    "\n",
    "The goal now is to make use of UMAP as a preprocessing step that one could potentially inject into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = umap.UMAP(n_neighbors=5, random_state=42).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we embedded to two dimensions, as before we can visualise the results to ensure that we are getting a potential benefit out of this approach. Note that the embedded training data can be accessed as the `.embedding_` attribute of the UMAP model, once we have fit it to some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tf.embedding_[:, 0], tf.embedding_[:, 1],\n",
    "            c=y_train,\n",
    "            cmap='Spectral',\n",
    "            s=5)\n",
    "\n",
    "plt.title('Embedding of the training set by UMAP', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks very promising! Most of the classes got very neatly separated, and that gives us some hope that it could help with classifier performance. It is worth noting that this was a *completely unsupervised* data transform.\n",
    "\n",
    "We can now train some new models (again an SVC and a KNN classifier) on the embedded training data. This looks exactly as before but now we pass it the embedded data. Note that calling `transform` on input identical to what the model was trained on will simply return the `embedding_` attribute, so sklearn pipelines will work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', gamma='scale').fit(tf.embedding_, y_train)\n",
    "knn = KNeighborsClassifier().fit(tf.embedding_, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to work with the test data, which none of the models (UMAP or the classifiers) have seen.\n",
    "To do this we use the standard sklearn API and make use of the `transform` method, this time handing it the new unseen test data. We will assign this to `test_embedding` so that we can take a closer look at the result of applying an existing UMAP model to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test_embedding = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the transform operations works very efficiently. Compared to some other transformers this is a little on the slow side, but it is fast enough for many uses. Note that as the size of the training and/or test sets increase, the performance will drop proportionally. It is also worth noting that the first call to `transform` may be slow due to the underlying Numba JIT (just-in-time) overhead – further runs will be very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test_embedding = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important question is what the transform did to our test data. In principle we have a new 2D representation of the test set. Ideally this should be based on the existing embedding of the training set. \n",
    "\n",
    "We can check this by visualising the data (since we are in two dimensions). A simple scatterplot as before will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_embedding[:, 0], test_embedding[:, 1],\n",
    "            c=y_test,\n",
    "            cmap='Spectral',\n",
    "            s=5)\n",
    "\n",
    "plt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\n",
    "plt.title('Embedding of the test set by UMAP', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look like what we expected: the test data has been embedded into two dimensions in the locations we should expect (by class) given the embedding of the training data visualised above. This means we can now evaluate the models that were trained on the embedded training data by handing them the newly transformed test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVC MCC: {matthews_corrcoef(svc.predict(tf.transform(X_test)), y_test):.5f}\")\n",
    "print(f\"KNN MCC: {matthews_corrcoef(knn.predict(tf.transform(X_test)), y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty good, with performances only slightly worse w.r.t. the plain test data. The point here is that we can use UMAP as if it were a standard sklearn transformer as part of an sklearn machine learning [pipeline](https://scikit-learn.org/stable/modules/compose.html#combining-estimators).\n",
    "\n",
    "Just for fun we can run the same experiments, but this time reducing to ten dimensions (which we can no longer visualise). In practice this will have little gain in this case – for the digits dataset, two dimensions is plenty for UMAP and more dimensions will not help. On the other hand, for more complex datasets where more dimensions may allow for a much more faithful embedding, it is worth noting that we are not restricted to only two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2 = umap.UMAP(n_neighbors=5, n_components=10, random_state=42).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf', gamma='scale').fit(tf2.embedding_, y_train)\n",
    "knn = KNeighborsClassifier().fit(tf2.embedding_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVC MCC: {matthews_corrcoef(svc.predict(tf2.transform(X_test)), y_test):.5f}\")\n",
    "print(f\"KNN MCC: {matthews_corrcoef(knn.predict(tf2.transform(X_test)), y_test):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that in fact we marginally improved the SVC score. However, for more interesting datasets a larger dimensional embedding might have a more significant gain - it is certainly worth exploring as one of the parameters in a grid search across a pipeline that includes UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "(partially abridged from [Visualising high-dimensional datasets using PCA and t-SNE in Python](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) and [Transforming New Data with UMAP](https://umap-learn.readthedocs.io/en/latest/transform.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rememebr that dimensionlaity reduction, you always lose some data but it can be really useful for data visualization.\n",
    "\n",
    "In general, t-SNE is not much used while PCA and UMAP are the main ones depending on the data at usage.\n",
    "PCA uses the notion of covariance while UMAP the one of manifolds. Both can project the data while t-SNE cannot do it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
